
<!DOCTYPE html>
<html lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml">
 <head>
    <meta charset="utf-8"/>
    <meta name="twitter:card" content="summary" />
    <meta property="og:url" content="https://arbml.github.io/Calliar/page/index.html" />
    <meta property="og:title" content="Calliar: An Online Handwritten Dataset for Arabic Calligraphy" />
    <meta property="og:description" content="Calligraphy is an essential part of the Arabic heritage and culture. It has been used in the past for the decoration of houses and mosques. Usually, such calligraphy is designed manually by experts with aesthetic insights. In the past few years, there has been a considerable effort to digitize such type of art by either taking a photo of decorated buildings or drawing them using digital devices. The latter is considered an online form where the drawing is tracked by recording the apparatus movement, an electronic pen for instance, on a screen. In the literature, there are many offline datasets collected with a diversity of Arabic styles for calligraphy. However, there is no available online dataset for Arabic calligraphy. In this paper, we illustrate our approach for the collection and annotation of an online dataset for Arabic calligraphy called Calliar that consists of 2,500 sentences. Calliar is annotated for stroke, character, word and sentence level prediction." />
    <meta property="og:image" content="https://raw.githubusercontent.com/ARBML/Calliar/main/media/sample_calliar_image_3.png" />

    <title>
        Calliar
    </title>

  <style type="text/css">
   :root {
    --small-thumb-border-radius: 2px;
    --larger-thumb-border-radius: 3px;
}

html {
  font-size: 14px;
  line-height: 1.6;
  font-family: Inter, system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
  text-size-adjust: 100%;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}

@media(min-width: 768px) {
  html {
    font-size: 16px;
  }
}

body {
    margin: 0px;
    padding: 0px;
}
.center {
    justify-content: center;
    margin:0px auto;
    display: block
}


.base-grid,
.n-header,
.n-byline,
.n-title,
.n-article,
.n-footer {
    display: grid;
    justify-items: stretch;
    grid-template-columns: [screen-start] 8px [page-start kicker-start text-start gutter-start middle-start] 1fr 1fr 1fr 1fr 1fr 1fr 1fr 1fr [text-end page-end gutter-end kicker-end middle-end] 8px [screen-end];
    grid-column-gap: 8px;
}

.grid {
  display: grid;
  grid-column-gap: 8px;
}

@media(min-width: 768px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 16px;
    }

    .grid {
        grid-column-gap: 16px;
    }
}

@media(min-width: 1000px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start] 50px [middle-start] 50px [text-start kicker-end] 50px 50px 50px 50px 50px 50px 50px 50px [text-end gutter-start] 50px [middle-end] 50px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 16px;
    }

    .grid {
        grid-column-gap: 16px;
    }
}

@media (min-width: 1180px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 32px;
    }
    .grid {
        grid-column-gap: 32px;
    }

}

.base-grid {
  grid-column: screen;
}

/* default grid column assignments */
.n-title > *  {
  grid-column: text;
}

.n-article > *  {
  grid-column: text;
}

.n-header {
    height: 0px;
}

.n-footer {
    height: 60px;
}

.n-title {
    padding: 4rem 0 2.5rem;
}

.l-page {
    grid-column: page;
}

.l-article {
    grid-column: text;
}

p {
  margin-top: 0;
  margin-bottom: 1em;
}

.pixelated {
    image-rendering: pixelated;
}

strong {
    font-weight: 600;
}

/*------------------------------------------------------------------*/
/* title */
.n-title h1 {
    font-family: "Barlow",system-ui,Arial,sans-serif;
    color:#082333;
    grid-column: text;
    font-size: 40px;
    font-weight: 600;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
    text-align: center;
}

@media (min-width: 768px) {
    .n-title h1 {
        font-size: 50px;
    }
}

/*------------------------------------------------------------------*/
/* article */
.n-article {
    color: rgb(33, 40, 53);
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    padding-top: 2rem;
}

.n-article h2 {
    contain: layout style;
    font-weight: 600;
    font-size: 24px;
    line-height: 1.25em;
    margin: 2rem 0 1.5rem 0;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding-bottom: 1rem;
}

@media (min-width: 768px) {
    .n-article {
        line-height: 1.7;
    }

    .n-article h2 {
        font-size: 36px;
    }
}

/*------------------------------------------------------------------*/
/* byline */

.n-byline {
  contain: style;
  overflow: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  font-size: 0.8rem;
  line-height: 1.8em;
  padding: 1.5rem 0;
  min-height: 1.8em;
}

.n-byline .byline {
  grid-column: text;
}

.byline {
    grid-template-columns: 1fr 1fr 1fr 1fr;
}

.grid {
    display: grid;
    grid-column-gap: 8px;
}

@media (min-width: 768px) {
.grid {
    grid-column-gap: 16px;
}
}

.n-byline p {
  margin: 0;
}

.n-byline h3 {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    margin: 0;
    text-transform: uppercase;
}
.n-byline .authors-affiliations {
  grid-column-end: span 2;
  grid-template-columns: 1fr 1fr;
}

ul.resources {
  grid-column-end: span 2;
  grid-template-columns: 1fr 1fr;
}

/*------------------------------------------------------------------*/
/* figures */
.figure {
    margin-top: 1.5rem;
    margin-bottom: 1rem;
}

figcaption, .figcaption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
}

ul.authors {
    list-style-type: none;
    padding: 0;
    margin: 0;
    text-align: center;
}

ul.resources {
    list-style-type: none;
    padding: 0;
    margin: 0;
    text-align: center;
    font-size: 20px;
}

ul.authors li {
    padding: 0.5rem 0.5rem;
    display: inline-block;
}

ul.resources li {
    padding: 0.5rem 0.5rem;
    display: inline-block;
    margin-left: 1.7rem;

}

ul.authors sup {
    color: rgb(126,126,126);
}

ul.authors.affiliations{
    margin-top: 0.3rem;
}




ul.resources li {
    color: rgb(126,126,126);
}

/* Download section columns.  This switches between two layouts::after

- two columns on larger viewport sizes: side-by-side paper thumb and links
- single column: no thumb
 */
.download-section {
    display: grid;
    grid-template-areas: "links";
}
.download-section h4 {
    margin-left: 2.5rem;
    display: block;
}
.download-thumb {
    grid-area: thumb;
    display: none;
}
.download-links {
    grid-area: links;
}
img.dropshadow {
    box-shadow: 0 1px 10px rgba(0,0,0, 0.3);
}

@media(min-width: 1180px) {
    .download-section {
        display: grid;
        grid-template-areas: "thumb links";
    }
    .download-thumb {
        display: block;
    }
}

/* For BibTeX */
pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

/* video caption */

.video {
    margin-top: 1.5rem;
    margin-bottom: 1.5rem;
}

.videocaption {
    display: flex;
    font-size: 16px;
    line-height: 1.5em;
    margin-bottom: 1rem;
    justify-content: center;
}
   .disable-selection {
         user-select: none;
    -moz-user-select: none; /* Firefox */
     -ms-user-select: none; /* Internet Explorer */
  -khtml-user-select: none; /* KHTML browsers (e.g. Konqueror) */
 -webkit-user-select: none; /* Chrome, Safari, and Opera */
 -webkit-touch-callout: none; /* Disable Android and iOS callouts*/
}

.hidden {
    display: none;
}

h3.figtitle {
    margin-top: 0;
    margin-bottom: 0;
}

.fig-title-line {
    grid-template-columns: 2fr 0.75fr;
}

.fig-thumb-image-row {
    grid-template-columns: 1fr 1fr;
    grid-template-rows: 1fr;
}

.fig-thumb-image-row-item {
    width: 100%;
    min-height: auto;
    border-radius: var(--small-thumb-border-radius);
}

.fig-dataset-button {
    border-color: rgba(0,0,0,0);
    border-width: 1px;
    border-style: solid;
    cursor: pointer;
    opacity: 0.6;
}

.fig-dataset-button.active {
    border-color: rgba(0,0,0,0.7);
    border-width: 1px;
    border-style: solid;
    opacity: 1.0;
}

.grid {
    display: grid;
    grid-column-gap: 8px;
}

.fig-3-image-row {
    margin-top: 1em;
    grid-template-columns: 1fr 1.3fr 1fr;
    grid-template-rows: 1fr;
}

.fig-3-image-item {
    justify-self: center;
    align-self: center;
    width: 100%;
    border-radius: var(--larger-thumb-border-radius);
}

/*---------------------------------------------------------------------*/
.fig-slider {
    grid-template-columns: auto 2fr;
    grid-template-rows: 1fr;
    margin-top: 1em;
    align-items: start;
    justify-content: center;
}

.fig-slider img.play_button {
    margin-right: 8px;
    cursor: pointer;
    justify-self: center;
}
.fig-slider svg {
    touch-action: none;
}

.fig-preload {
    display: none;
}
.button {
  background-color: #4CAF50; /* Green */
  border: none;
  color: white;
  padding: 10px 15px;
  text-align: center;
  text-decoration: none;
  display: inline-block;
  font-size: 12px;
  margin: 4px 2px;
  transition-duration: 0.4s;
  cursor: pointer;
}

.button2 {
  background-color: white; 
  color: black; 
  border: 1px solid #008CBA;
  margin-top: 15px;
}

.button2:hover {
  background-color: #008CBA;
  color: white;
  
}

.button2:disabled {
  border: 1px solid #999999;
  background-color: #cccccc;
  color: #666666;
}



/*---------------------------------------------------------------------*/
  </style>
  <!-- inline stylesheet files into the above <style> element -->
  <link href="https://fonts.googleapis.com/css?family=Montserrat|Segoe+UI" rel="stylesheet"/>
 </head>
 <body>

    <select onchange="if (this.value) window.location.href=this.value">
        <option value="index.html">English</option>
        <option value="index_ar.html">Arabic</option>

    </select>
  <div class="n-header">
  </div>
  <div class="n-title">
   <h1>
    Calliar: An Online Handwritten Dataset for Arabic Calligraphy
   </h1>
  </div>
  <div class="n-byline">
   <div class="byline">
    <ul class="authors">
     <li>
        Zaid Alyafeai
      <sup>
       1
      </sup>
     </li>

     <li>
        Maged S. Al-shaibani
      <sup>
       1
      </sup>
     </li>

     <li>
        Mustafa Ghaleb
      <sup>
       1
      </sup>
     </li>

     <li>
        Yousif Ahmed Al-Wajih
      <sup>
       1
      </sup>
     </li>

    </ul>
    <ul class="authors affiliations">
     <li>
      <sup>
       1
      </sup>
      KFUPM
     </li>
    </ul>
    <ul class="resources">
        <li>
         <a href = "https://arxiv.org/abs/2106.10745">arXiv</a> 
        </li>
        <li>
            <a href = "https://github.com/ARBML/Calliar">GitHub</a>
        </li>
       </ul>

   </div>
  </div>
  <div class="n-article">
   <div class="l-article">
    <img src = "https://raw.githubusercontent.com/ARBML/Calliar/main/media/sample_calliar_image_3.png" width="100%"> </img>
    </div>
   <h2 id="abstract">
    Abstract
   </h2>
   <p style = "text-align: justify;">
    Calligraphy is an essential part of the Arabic heritage and culture. It has been used in the past for the decoration of houses and mosques. Usually, such calligraphy is designed manually by experts with aesthetic insights. In the past few years, there has been a considerable effort to digitize such type of art by either taking a photo of decorated buildings or drawing them using digital devices. The latter is considered an online form where the drawing is tracked by recording the apparatus movement, an electronic pen for instance, on a screen. In the literature, there are many offline datasets collected with a diversity of Arabic styles for calligraphy. However, there is no available online dataset for Arabic calligraphy. In this paper, we illustrate our approach for the collection and annotation of an online dataset for Arabic calligraphy called Calliar that consists of 2,500 sentences. Calliar is annotated for stroke, character, word and sentence level prediction.   </p>
    
    <h2>
        Demo
    </h2>
    <p style = "text-align: justify;">
        You can generate an animation of a subset of the data set by clicking the button below. Note that this runs directly in the browser hence we only used 100 samples for demonstration.  
        The 100 json files where converted to a minified version where all points were converted to integers with no stroke annotations. At each press of generate a new 
        sketch is rendered where each stroke is drawn in different color. The annimation speed is propotional to the length of the stroke. You will see that some 
        strokes might have fast animations and some slow. 
    </p>

    <div style = "text-align: center; margin: 0 auto;">
        <div id="canvas" style = "border-width: thin;border-color: black;border-style:solid"> 
        </div>

        <canvas id="myCanvas" width = "600px" height = "600px"  style="border:1px solid #000000;" resize hidden></canvas>

        <button class = "button button2" id = "generate">
            Generate
        </button> <input type="range" id = 'slider' min="1" max="10" value="3">
    </div> 

    <h2 id="videos">
        Collection & Annotation
    </h2>
    <p style = "text-align: justify;">
    The collection procedure was done during a span of months. We used some of the images that we collected to generate some calligraphy as shown in the image below. One of the main websites 
    that we used initially is  <a href= "https://www.namearabic.com/">namearabic</a>. Luckily this dataset had some text annotation and we had only to spend 
    a few hours to annotate with drawing a few hundred images. For other datasets we had to annotate the images with text then with sketch drawing.
    
    The annotation procedure took a few weeks. We used two Samsung Galaxy Tab S6 tablets as the main devices for the annotation procedure. It took two main phases: the first one 
    annotates each image with a text. In the second phase we used the pen of the tablet to draw the annotation. We repeated these steps multiple times to increase the size of the dataset. Because 
    the intial dataset was 600x600 we decided to fix the max dimension to 600 and rescale the other dimension accordingly to preserve the spect ratio. During the annotation process 
    we faced many problems like empty annotations, repeated strokes and problems with the touch screen which created wrong annotations. To deal with these problems, 
    we made a lot of back and forth verification steps from directly annotating and then animating the results immediately using Python. We would annotate some images then remove them because they had some problems. 
    One sanity check that used is to compare the text annoation with the json stroke annotation. This gives us some quick discovery of empty annotations. The other problems are difficult to deal with especially the problem 
    of having wrong drawings by the tablet. We relied on post processing to get rid of such annotations by comparing the variance of the points in one stroke drawing. 

    </p>
    <img src = "https://raw.githubusercontent.com/zaidalyafeai/sgan/master/calligraphyv2.PNG" width="100%"> </img>
    
       

    <h2 id="videos">
        Creative Applications
    </h2>
    <p style = "text-align: justify;">
        There is a lot of controversy in the field of AI whether we can use it to create some intelligent and artistic programs.
        In the last few years there has been a lot of research in applying AI creatively like Style Transfer<sup>[1]</sup>,
        Deep Dream<sup>[2]</sup>, GauGAN<sup>[3]</sup>, etc. Most of these technologies apply for images. These applications apply mostly for computer vision, on the other
        hand it is much more difficult in natural language processing (NLP). The intrinsic difficulty lies in the complexity 
        of modelling language, let alone creating some creative applications. One of the most interesting applications are sketch generation. 
        One of the most interesting papers is the one Alex Graves <a href ="https://arxiv.org/abs/1308.0850">Generating Sequences With Recurrent Neural Networks</a>. 
        The paper assumes the existance of an online corpus for generation stroke sequences for Engish. Building upon that, there has been many papers in that field like
        sketchRNN<sup>[4]</sup>, GANwriting<sup>[5]</sup>, Scrabble-GANs<sup>[6]</sup>, DF-GANs<sup>[7]</sup>, BézierSketch<sup>[8]</sup> and DoodlerGAN<sup>[9]</sup>. Most of these papers work on English and it is needless to mention how simple English is 
        compared to other languages like Arabic. The complexity of Arabic raises from the cursive nature of connecting letters together. Not to mention the long history 
        of Arabic calligraphy which is used extensively nowadays. The different styles of Arabic calligraphy joint with the freedom of drawing some letters makes the problem much harder. 

        Being the only dataset that collect online stroke information for different calligraphic styles, this opens the door for many applications. 
        The difficulty still lies in modelling natural language from the strokes. There is a trade-off between creating nice strokes and generating sensible language. In the literature, 
        most of the generated strokes are usually words not sentences. In Arabic calligraphy, creating words is not an interesting problem. Much of the beauty is from following style in addition to 
        conditioning the words to specific text view which is usually a circle like in Diwani and square like in Kufi.  A proper contribution that we want to achieve in the future is to generate calligraphy 
        conditioned on style and text. 
    </p>
    <h2 id="videos">
        Tweet
       </h2>

        <blockquote class="twitter-tweet tw-align-center"><p lang="en" dir="ltr">Pleased to announce Calliar, the first online dataset for Arabic Calligraphy. Joint work with <a href="https://twitter.com/_MagedSaeed_?ref_src=twsrc%5Etfw">@_MagedSaeed_</a> <a href="https://twitter.com/alwaridi?ref_src=twsrc%5Etfw">@alwaridi</a> and Yousif Al-Wajih. <br>Paper: <a href="https://t.co/q5mwcP6roa">https://t.co/q5mwcP6roa</a><br>Code &amp; data: <a href="https://t.co/tDCPJRYUcl">https://t.co/tDCPJRYUcl</a><br>Colab: <a href="https://t.co/46yD0V3wht">https://t.co/46yD0V3wht</a> <a href="https://t.co/qbbb4tZJ6l">pic.twitter.com/qbbb4tZJ6l</a></p>&mdash; Zaid زيد (@zaidalyafeai) <a href="https://twitter.com/zaidalyafeai/status/1407383686786527234?ref_src=twsrc%5Etfw">June 22, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

   <h2 id="videos">
    Videos
   </h2>
  
   <div class="l-article">
    <video controls="" loop="" width="100%">
     <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
     <source src="https://user-images.githubusercontent.com/15667714/122690255-1acff980-d231-11eb-9752-730d8024041e.mp4" type="video/mp4"/>
    </video>
    <div class="videocaption">
     <div>
      <strong>
       Video 1a:
      </strong>
      Animating various calligraphy styles. 
     </div>
    </div>
   </div>

   <div class="l-article">
    <video controls="" loop="" width="100%">
     <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
     <source src="https://user-images.githubusercontent.com/15667714/122690261-1d325380-d231-11eb-99ab-158455483561.mp4" type="video/mp4"/>
    </video>
    <div class="videocaption">
     <div>
      <strong>
       Video 1b:
      </strong>
      Animating various calligraphy styles. 
     </div>
    </div>
   </div>

   <div class="l-article">
    <video controls="" loop="" width="100%">
     <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
     <source src="https://user-images.githubusercontent.com/15667714/122690258-1c012680-d231-11eb-98d0-4c89afd8aba3.mp4" type="video/mp4"/>
    </video>
    <div class="videocaption">
     <div>
      <strong>
       Video 3:
      </strong>
      Animating the same phrase using multiple styles. 
     </div>
    </div>
   </div>

  

   <div class="l-article">
    <video controls="" loop="" width="100%">
     <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
     <source src="https://user-images.githubusercontent.com/15667714/122690265-1efc1700-d231-11eb-9830-328725a82c11.mp4" type="video/mp4"/>
    </video>
    <div class="videocaption">
     <div>
      <strong>
       Video 3:
      </strong>
      Animating complicated calligraphic styles. 
     </div>
    </div>
   </div>

   <pre><code>
    @misc{alyafeai2021calliar,
        title={Calliar: An Online Handwritten Dataset for Arabic Calligraphy}, 
        author={Zaid Alyafeai and Maged S. Al-shaibani and Mustafa Ghaleb and Yousif Ahmed Al-Wajih},
        year={2021},
        eprint={2106.10745},
        archivePrefix={arXiv},
        primaryClass={cs.CL}
  }</code></pre>

   <h2>
    References
   </h2>
   <ol>
    <li>Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. "Perceptual losses for real-time style transfer and super-resolution." European conference on computer vision. Springer, Cham, 2016.</li>
    <li>Mordvintsev, Alexander, Christopher Olah, and Mike Tyka. "Inceptionism: Going deeper into neural networks." (2015).</li>
    <li>Park, Taesung, et al. "GauGAN: semantic image synthesis with spatially adaptive normalization." ACM SIGGRAPH 2019 Real-Time Live!. 2019. 1-1.</li>
    <li>Ha, David, and Douglas Eck. "A neural representation of sketch drawings." arXiv preprint arXiv:1704.03477 (2017).</li>
    <li>Kang, Lei, et al. "GANwriting: Content-conditioned generation of styled handwritten word images." European Conference on Computer Vision. Springer, Cham, 2020.</li>
    <li>Fogel, Sharon, et al. "ScrabbleGAN: semi-supervised varying length handwritten text generation." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</li>
    <li>Tao, Ming, et al. "Df-gan: Deep fusion generative adversarial networks for text-to-image synthesis." arXiv preprint arXiv:2008.05865 (2020).</li>
    <li>Das, Ayan, et al. "BézierSketch: A generative model for scalable vector sketches." arXiv preprint arXiv:2007.02190 (2020).</li>
    <li>Ge, Songwei, et al. "Creative Sketch Generation." arXiv preprint arXiv:2011.10039 (2020).</li>
   </ol>  
  
  

  <div>
    This website uses the template from <a href="https://nvlabs.github.io/alias-free-gan/">alias-free-gan</a>. 
 </div>


<script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.1.0/raphael-min.js"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/paper.js/0.11.8/paper-full.min.js"></script>
<script src = "json_sm.js"></script>
<script src = "index.js"></script>
  